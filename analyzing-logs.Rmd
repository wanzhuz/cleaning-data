---
title: "Reading Log Files"
author: "Wanzhu Zheng"
date: "May 2023"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
```

## Reading Log File

We start with a file that contains 5 logging files. We first read the
lines from the file into a character vector and get rid of the titles of
each logging file.

```{r}
file = "/Users/wanzhu_zheng/Downloads/MergedAuth.log"
lines = readLines(file, warn = FALSE)
l = lines[!grepl("^#\\s*", lines) & lines != ""]
colnames = c("date-time", "logging-host", "app", "PID", "message")
```

Then, we build a regular expression that matches every line in the file
by employing capture groups. We then use gregexpr() and the capture
groups to extract necessary information to create our dataframe.

We first start by capturing the date-time:

```{r}
d = grepl("^([a-zA-z]+ [ |0-9]+ [0-9:]+) ", l, perl = TRUE)
table(d)
```

Since all the lines returned TRUE, we move on to match the logging-host:

```{r}
d = grepl("^([a-zA-z]+ [ |0-9]+ [0-9:]+) ([^ ]+) ", l, perl = TRUE)
table(d)
```

Again, we returned TRUE for all the lines. Now, we move on to match the
app name. Since the app and the PID are not separated by a space, we
don't separate our capture group either.

```{r}
d = grepl("^([a-zA-z]+ [ |0-9]+ [0-9:]+) ([^ ]+) ([^\\[|^:]+)([\\[0-9]+\\]:?|:|[\\[0-9]+\\]:?:) ", l, perl = TRUE)
table(d)
```

Now, notice that some lines in our log file are formatted differently.

```{r echo=FALSE}
"Jul  1 09:29:02 calvisitor-10-105-160-95 sandboxd[129] ([31211]): 
com.apple.Addres(31211) deny network-outbound /private/var/run/mDNSResponder"
```

For example, the above message has another set of PID ([31211]). We use
gsub() to move that behind the ":" and treat it as part of the message.
We then match the remainder of the line.

```{r}
ll = gsub("^([a-zA-z]+ [ |0-9]+ [0-9:]+) ([^ ]+) ([^\\[|^:]+)([\\[0-9]+\\]:?|:)) \\(", "\\1: (", l)
d = grepl("^([a-zA-z]+ [ |0-9]+ [0-9:]+) ([^ ]+) ([^\\[|^:]+)([\\[0-9]+\\]:?|:|[\\[0-9]+\\]:?:) (.*)$", ll, perl = TRUE)
table(d)
```

## Verification

Although d returns TRUE for all our lines, we still need to verify that
we matched the correct sub-patterns. To do so, we use gregexpr().

```{r}
m = gregexpr("^([a-zA-z]+ [ |0-9]+ [0-9:]+) ([^ ]+) ([^\\[|^:]+)([\\[0-9]+\\]:?|:|[\\[0-9]+\\]:?:) (.*)$", ll, perl = TRUE)
```

We check how many matches there are for each line:

```{r}
table(sapply(m, length)) # check matches for each line
```

This means that every line has one match, which is what we expected.
Now, let's check the start of the string. This should also be length 1:

```{r}
table(sapply(m, "[", 1) == 1) # check starting position
```

Let's verify that the length of the entire match is the number of
characters in each line:

```{r}
mlen = sapply(m, function(x) attr(x, "match.length"))
table(mlen)
```

```{r}
table(mlen == nchar(ll)) # verify length of match = number of char in each string
```

Now, we focus on the sub-patterns and search for where they start and
end and extract their values. We first look at the first element of m:

```{r}
m[[1]]
```

Note that the attributes "capture.start" and "capture.length" give us
the start of each capture group matched in the string and the number of
characters that were matched in that capture group.

We use substring to extract these:

```{r}
# check for substrings
s = attr(m[[1]], "capture.start")
substring(ll[1], s, s + attr(m[[1]], "capture.length"))
```

Note that we have an extra "[" in our app and extra "[]:" in our PID. We
will clean that up later when we create our dataframe. For now, we loop
over every line in ll and m to extract the substrings for the capture
groups.

```{r}
matr = mapply(function(str, match) {
  s = attr(match, "capture.start")
  substring(str, s, s + attr(match, "capture.length"))
}, ll, m)
```

This returns a 99960 by 5 matrix. We transpose it and convert it to a
dataframe:

```{r}
# transpose into dataframe
df = as.data.frame(t(matr))
```

We clean up the dataframe by getting rid of unnecessary symbols we noted
before. We also get rid of the row names and and rename the columns.

```{r}
# clean up dataframe
names(df) = colnames # rename cols
rownames(df) = NULL # get rid of row names
df$app = gsub("\\[|:", "", df$app) # get rid of unnecessary characters
df$PID = gsub("\\[|\\]|:| ", "", df$PID) # get rid of unnecessary characters
df[df == ""] = NA
df[nchar(as.character(df)) == 0] = NA
```

Let's check the dimensions of our dataframe and verify it's the same as
the number of lines in our log file.

```{r}
dim(df)
stopifnot(nrow(df) == length(ll))
```

Check column names:

```{r}
names(df)
```

Now, we create a new column that specifies which log file each line is
from. To do so, we use ifelse() statements. Then, check dimensions of
our dataframe again to make sure we added just one column.

```{r}
df$`log-file` = with(df, ifelse(`logging-host` == "ip-172-31-27-153 ", "auth.log",
                      ifelse(`logging-host` == "ip-10-77-20-248 ", "auth2.log",
                             ifelse(`logging-host` == "combo ", "Linux_2k.log",
                                    ifelse(`logging-host` == "LabSZ ", "SSH_2k.log", 
                                           "Mac_2k.log")))))
dim(df)
```

Next, we verify that our PID are all numbers.

```{r}
df$PID = as.numeric(df$PID)
is.numeric(df$PID)
```

We check how many lines are in each log file by using grepl().

```{r}
auth = table(grepl("auth\\.log", df$`log-file`))[["TRUE"]]
auth
auth2 = table(grepl("auth2\\.log", df$`log-file`))[["TRUE"]]
auth2
linux = table(grepl("Linux_2k\\.log", df$`log-file`))[["TRUE"]]
linux
mac = table(grepl("Mac_2k\\.log", df$`log-file`))[["TRUE"]]
mac
ssh = table(grepl("SSH_2k\\.log", df$`log-file`))[["TRUE"]]
ssh
```

Check that the number of lines in each log file add up to the total
number of lines in the master log file.

```{r}
auth + auth2 + linux + mac + ssh == length(df$`log-file`)
```

Now, we subset each log file to create 5 different dataframes.

```{r}
auth_df = subset(df, df$`log-file` == "auth.log")
auth2_df = subset(df, df$`log-file` == "auth2.log")
linux_df = subset(df, df$`log-file` == "Linux_2k.log")
mac_df = subset(df, df$`log-file` == "Mac_2k.log")
ssh_df = subset(df, df$`log-file` == "SSH_2k.log")
```

Now, we change the dates to datetime format using as.POSIXct(). Then we
call range() to give us the beginning and ending date of each log file.
Finally, we return the number of days each log file spans.

For auth.log:

```{r}
auth_df$`date-time` = as.POSIXct(auth_df$`date-time`, format="%b %d %H:%M:%S") 
range(auth_df$`date-time`) # range of dates
as.numeric(ceiling(as.numeric(difftime(max(auth_df$`date-time`), 
                                                       min(auth_df$`date-time`), units = "days"))))
```

For auth2.log:

```{r}
# auth2.log
auth2_df$`date-time` = as.POSIXct(auth2_df$`date-time`, format="%b %d %H:%M:%S") 
range(auth2_df$`date-time`) # range of dates
as.numeric(ceiling(as.numeric(difftime(max(auth2_df$`date-time`), 
                                       min(auth2_df$`date-time`), units = "days"))))
```

For loghub/Linux/Linux_2k.log:

```{r}
# linux
linux_df$`date-time` = as.POSIXct(linux_df$`date-time`, format="%b %d %H:%M:%S") 
range(linux_df$`date-time`) # range of dates
as.numeric(ceiling(as.numeric(difftime(max(linux_df$`date-time`), 
                                       min(linux_df$`date-time`), units = "days"))))
```

For loghub/Mac/Mac_2k.log:

```{r}
# mac
mac_df$`date-time` = as.POSIXct(mac_df$`date-time`, format="%b %d %H:%M:%S") 
range(mac_df$`date-time`) # range of dates
as.numeric(ceiling(as.numeric(difftime(max(mac_df$`date-time`), 
                                       min(mac_df$`date-time`), units = "days"))))
```

For loghub/OpenSSH/SSH_2k.log:

```{r}
# ssh
ssh_df$`date-time` = as.POSIXct(mac_df$`date-time`, format="%b %d %H:%M:%S") 
range(ssh_df$`date-time`) # range of dates
as.numeric(ceiling(as.numeric(difftime(max(ssh_df$`date-time`), 
                                       min(ssh_df$`date-time`), units = "days"))))
```

Now, we check if the app names contain numbers.

```{r}
df[grepl("[0-9]+", df$app), "app"]
```

Notice that the app names do contain numbers. The numbers indicate what
is happening to the units. For example, syslogd 1.4.1 suggests that
there has been an unexpected reboot/restart of the system. BezelServices
255.10 suggests that there is an error in behavior of a UI surface on
Mac OSX.

Now, we identify if all hosts are the same for each log file .

```{r}
if (all(auth_df$`logging-host` == auth_df$`logging-host`[1])) {
  print("All values in logging host are the same for auth.log")
} else {
  print("Not all values in logging host are the same for auth.log")
}
```

We repeat this for all 5 log files.

```{r}
if (all(auth2_df$`logging-host` == auth2_df$`logging-host`[1])) {
  print("All values in logging host are the same for auth2.log")
} else {
  print("Not all values in logging host are the same for auth2.log")
}

if (all(linux_df$`logging-host` == linux_df$`logging-host`[1])) {
  print("All values in logging host are the same for linux.log")
} else {
  print("Not all values in logging host are the same for linux.log")
}

if (all(mac_df$`logging-host` == mac_df$`logging-host`[1])) {
  print("All values in logging host are the same for mac.log")
} else {
  print("Not all values in logging host are the same for mac.log")
}

if (all(ssh_df$`logging-host` == ssh_df$`logging-host`[1])) {
  print("All values in logging host are the same for ssh.log")
} else {
  print("Not all values in logging host are the same for ssh.log")
}
```

Notice that not all values in the logging host are the same for mac.log.
We can double check this by returning the first couple rows of the
dataframe:

```{r}
head(mac_df$`logging-host`)
```

We return the most common app name for each logging host by using the
aggregate() function. This will give us a one-to-one correspondence list
of most used app to logging host.

```{r}
aggregate(app ~ `logging-host`, data = df, FUN = function(x) {
  names(which.max(table(x)))
})
```

## Logins- Valid and Invalid

We find successful log-ins by first identifying keywords that indicate a
successful log-in.

```{r}
v = subset(df, grepl("accepted|new session|^connection|^successful", message, ignore.case = TRUE))
```

Then, we extract the IP address and the user of each message with
regular expressions. Any messages that don't contain an IP address or
user will be indicated with NA.

```{r}
v$IP = str_extract_all(v$message, "(\\d{1,3}\\.){3}\\d{1,3}")
v$IP = sapply(v$IP, function(x) ifelse(is.character(x) & length(x) > 1, x[[1]], x))
v$user = str_extract_all(v$message, "(?<=for )[a-zA-Z0-9]+(\\_[a-zA-Z]+)?(\\_[0-9]+)?|(?<=user )[a-zA-Z0-9]+(\\_[a-zA-Z]+)?(\\_[0-9]+)?")
v$user = sapply(v$user, function(x) ifelse(is.character(x) & length(x) > 1, x[[1]], x))
head(v)
```

Now, we do the same for all invalid log-ins. First, we identify keywords
that will indicate that a log-in was unsuccessful.

```{r}
inv = subset(df, grepl("invalid|failure|error|bad", message, ignore.case = TRUE))
```

Similarly, we extract the IP address and the user of each message with
regular expressions. Any messages that don't contain an IP address or
user will be indicated with NA.

```{r}
inv$IP = str_extract_all(inv$message, "(\\d{1,3}\\.){3}\\d{1,3}")
inv$IP = sapply(inv$IP, function(x) ifelse(is.character(x) & length(x) > 1, x[[2]], x))
inv$user = str_extract_all(inv$message, "(?<= user )[a-zA-Z0-9-.]+|(?<=by )[a-zA-Z0-9-.]+|(?<=for )[a-zA-Z0-9-.]+|(?<= user=)[a-zA-Z0-9-.]+")
inv$user = sapply(inv$user, function(x) ifelse(is.character(x) & length(x) > 1, x[[1]], x))
head(inv)
```

Check if IP addresses for invalid log-ins also had valid log-ins

```{r}
common_ip <- intersect(v$IP, inv$IP)
length(common_ip)
head(common_ip)
```

This tells us that there are 347 common IP addresses between valid and
invalid log-ins. We return the first 6 IP addresses.

Now, we check to see if there are there multiple invalid user log-ins
from the same IP addresses.

```{r}
users_by_ip = aggregate(user ~ IP, data = inv, FUN = function(x) 
  {paste(unique(x), collapse = ", ")})
head(users_by_ip)
```

This returns a table with the IP addresses and their associated users in
a list separated by commas. For the first 6 IP addresses, we see that
some of them are connected to more than one user.

Now, we check to see if there are multiple IPs using the same invalid
log-in.

```{r}
ip_by_users = aggregate(IP ~ user, data = inv, FUN = function(x) 
  {paste(unique(x), collapse = ", ")})
head(ip_by_users)
```

Alternatively, this returns a table with the username and their
associated IP's in a list separated by commas. For the first 6 users, we
see that some of them are connected to more than one IP address.

We want to see if these IP addresses are related in any way. One way to
do this would be to check their domains and identify any recurring
domains for each user.

First, we write a function that gets the IP domain:

```{r}
# function gets domain from IP addresses
get_domain = function(ip) {
  sub("\\.[^\\.]*$", "", ip)
}
```

Then, we find common domains:

```{r}
# find common domains
ip_by_users$domain = sapply(strsplit(as.character(ip_by_users$IP), ", "),
                             function(ips) paste0(sapply(ips, get_domain), 
                                                  collapse = ", "))
```

Finally, we return the frequency each domain appears in a table. This
will tell us which IP addresses were the most frequently used, since
they come from the same domain, just a different machine.

```{r}
domains = unlist(strsplit(ip_by_users$domain, ", "))
freq = table(domains)
freq = freq[order(freq, decreasing = TRUE)]
head(freq)
```

Now, we check which IP address had too many authentication failures.
There are two messages that could mean "too many authentication
failures."

```{r echo=FALSE}
"Mar 27 14:01:39 ip-10-77-20-248 sshd[2938]: Disconnecting: Too many 
authentication failures [preauth]"

"Mar 27 14:54:58 ip-10-77-20-248 sshd[2967]: error: maximum authentication 
attempts exceeded for invalid user support from 95.152.57.58 
port 53679 ssh2 [preauth]"
```

We read in both to our dataframe:

```{r}
fail = grep("too many authentication failures|error: maximum authentication 
            attempts exceeded", inv$message, ignore.case = TRUE)
ip_fail = sort(inv$IP[fail], na.last = TRUE)
length(ip_fail)
head(ip_fail)
```

This tells us that 2956 IP addresses had too many authentication
failures. We return the first 6 IP addresses to verify that these are
true.

## Sudo Commands

Sudo commands take the form of:

```{r echo=FALSE}
"Mar 27 15:49:20 ip-10-77-20-248 sudo:   ubuntu : TTY=pts/0 ; PWD=/tmp ; USER=root ; COMMAND=/usr/bin/dpkg -i filebeat-5.3.0-amd64.deb"
```

First, we create a dataframe of only sudo commands:

```{r}
sudo = subset(df, grepl("sudo", app, ignore.case = TRUE))
```

The executable/program follow the COMMAND= line. Thus, we use regular
expressions to extract the information after COMMAND=:

```{r}
sudo$executable = str_extract(sudo$message, "(?<=COMMAND=)/\\S+")
sudo = sudo[!is.na(sudo$executable), ] # cleans up dataframe by grouping open/close into one session
head(sudo$executable)
```

Similarly, the users of sudo follow the USER= line. Thus, we use regular
expressions to extract the information after USER=:

```{r}
sudo$user = str_extract(sudo$message, "(?<=USER=)\\S+")
head(sudo$user)
```

Now, we clean up the dataframe by only keeping the columns we want and
also renaming the columns to be more accurate to our situation.

```{r}
sudo = subset(sudo, select = c("logging-host", "app", "executable", "user")) # keep useful columns
names(sudo)[1] = "machine"
head(sudo)
```
